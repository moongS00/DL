{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64020933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a518f5c",
   "metadata": {},
   "source": [
    "# 자동미분\n",
    "\n",
    "`autograd`는 PyTorch에서 핵심적인 기능을 담당하는 하부 패키지이다. \n",
    "\n",
    "autograd는 텐서의 연산에 대해 **자동으로 미분값을 구해주는 기능**을 한다.\n",
    "\n",
    "- 텐서 자료를 생성할 때, `requires_grad`인수를 `True`로 설정하거나 \n",
    "- `.requires_grad_(True)`를 실행하면\n",
    "\n",
    "그 텐서에 행해지는 **모든 연산에 대한 미분값**을 계산한다. \n",
    "\n",
    "계산을 멈추고 싶으면 `.detach()`함수나 with을 이용해 `torch.no_grad()`를  이용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7036120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6687, 0.0309],\n",
      "        [0.0607, 0.5103]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7931fe",
   "metadata": {},
   "source": [
    "다음으로 이 x에 연산을 수행한다. 다음 코드의 y는 연산의 결과이므로 미분 함수를 가진다. <br>\n",
    "`grad_fn`속성을 출력해 미분 함수를 확인 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abe6a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8120, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = torch.sum(x * 3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f81d762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object at 0x00000240DC79DF30>\n"
     ]
    }
   ],
   "source": [
    "print( y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb09b7fb",
   "metadata": {},
   "source": [
    "`y.backward()` 함수를 실행하면 x의 미분값이 자동으로 갱신된다. x의 `grad`속성을 확인하면 미분값이 들어 있는 것을 확인 할 수 있다. y를 구하기 위한 x의 연산을 수식으로 쓰면 다음과 같다. \n",
    "\n",
    "$$\n",
    "y = \\displaystyle\\sum_{i=1}^4 3 \\times x_i\n",
    "$$\n",
    "\n",
    "이를 $x_i$에 대해 미분 하면 미분 함수는 다음과 같다. \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial y}{\\partial x_i} = 3\n",
    "$$\n",
    "\n",
    "실제 미분값과 같은지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24f42f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f007e9",
   "metadata": {},
   "source": [
    "- `backward()`함수는 자동으로 미분값을 계산해 <br> `requires_grad`인수가 True로 설정된 변수의 `grad`속성의 값을 갱신한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c92dcd",
   "metadata": {},
   "source": [
    "- `retain_graph` 미분을 연산하기 위해서 사용했던 임시 그래프를 유지 할 것인가를 설정하는 것이다.<br> \n",
    "기본값은 False로 설정되어 있지만 동일한 연산에 대해 여러번 미분을 계산하기 위해서는 <br>\n",
    "True로 설정되어 있어야한다.(`tf.GradientTape`에서 `persistent`와 같음)<br>\n",
    "<br>\n",
    "- `torch.autograd.grad()` 함수를 사용해 `tf.GradientTape`처럼 사용할 수도 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a6b74b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3.],\n",
       "         [3., 3.]]),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f6c96",
   "metadata": {},
   "source": [
    "---\n",
    "상황에 따라 특정 연산에는 미분값을 계산하고 싶지 않은 경우에는 \n",
    "\n",
    " - `.detach()`함수\n",
    " - `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c8fee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6612, 0.5077],\n",
       "        [0.5152, 0.6249]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.grad)\n",
    "x_d = x.detach()\n",
    "torch.sigmoid(x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd9e3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(x_d.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e4b8a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "with torch.no_grad():\n",
    "    x_d2 = torch.sigmoid(x)\n",
    "    print(x_d2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe72953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
