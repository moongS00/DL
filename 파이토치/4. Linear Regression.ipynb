{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e86c6a8",
   "metadata": {},
   "source": [
    "# PyTorch 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31269f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x260eb92cf70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3fdd530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gktna\\miniconda3\\envs\\soo\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>const</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM   ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "501  0.06263  0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527  0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076  0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959  0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741  0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  const  \n",
       "501     21.0  391.99   9.67    1.0  \n",
       "502     21.0  396.90   9.08    1.0  \n",
       "503     21.0  396.90   5.64    1.0  \n",
       "504     21.0  393.45   6.48    1.0  \n",
       "505     21.0  396.90   7.88    1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['const'] = np.ones(df.shape[0])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6f512",
   "metadata": {},
   "source": [
    "# 1.\n",
    "\n",
    "$X$ 를 Feature, ,$w$를 가중치 벡터, $y$를 Target이라 하자. \n",
    "\n",
    "$X^T X$의 역행렬이 존재 한다고 가정했을 때,  \n",
    "\n",
    "아래의 식을 이용해 $w$의 추정치 $w^*$를 구해봅시다. (PyTorch 연산 사용)\n",
    "\n",
    "$$\n",
    "w^{\\ast} = (X^TX)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c5498b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7619728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(df.values)\n",
    "y = torch.tensor(boston.target).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31626d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([506, 14]), torch.Size([506, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb477dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 506])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XT = torch.transpose(x, 0, 1)\n",
    "XT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ce4f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0801e-01],\n",
       "        [ 4.6420e-02],\n",
       "        [ 2.0559e-02],\n",
       "        [ 2.6867e+00],\n",
       "        [-1.7767e+01],\n",
       "        [ 3.8099e+00],\n",
       "        [ 6.9222e-04],\n",
       "        [-1.4756e+00],\n",
       "        [ 3.0605e-01],\n",
       "        [-1.2335e-02],\n",
       "        [-9.5275e-01],\n",
       "        [ 9.3117e-03],\n",
       "        [-5.2476e-01],\n",
       "        [ 3.6459e+01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.matmul(torch.mm(torch.linalg.inv(torch.matmul(XT, x)), XT), y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef8a8a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측한 집값 : tensor([18.4061], dtype=torch.float64) 실제 집값 : 18.2\n"
     ]
    }
   ],
   "source": [
    "y_pred = torch.mm(x, w)\n",
    "\n",
    "print(\"예측한 집값 :\", y_pred[19], \"실제 집값 :\", boston.target[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b49b37",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Gradient descent 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d14dd0",
   "metadata": {},
   "source": [
    "#### w, b 설정 & loss 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "441ea123",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand((14, 1), dtype=torch.float64, requires_grad=True)\n",
    "b = torch.rand((1, 1), dtype=torch.float64, requires_grad=True)  \n",
    "    # const 컬럼을 추가했기 때문에 b가 꼭 필요한건 아님!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cb93351",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.mm(w) + b\n",
    "loss = torch.mean((z - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6fddb",
   "metadata": {},
   "source": [
    "#### 미분값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d022599",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fd9e073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.6543e+03],\n",
       "         [1.1592e+04],\n",
       "         [1.2824e+04],\n",
       "         [7.4684e+01],\n",
       "         [6.1311e+02],\n",
       "         [6.7807e+03],\n",
       "         [7.6970e+04],\n",
       "         [3.9565e+03],\n",
       "         [1.1487e+04],\n",
       "         [4.6771e+05],\n",
       "         [2.0181e+04],\n",
       "         [3.9043e+05],\n",
       "         [1.4341e+04],\n",
       "         [1.0850e+03]], dtype=torch.float64),\n",
       " tensor([[1084.9567]], dtype=torch.float64))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b82ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(303557.1678, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8a1f7",
   "metadata": {},
   "source": [
    "- 그냥 loss 값만 출력하고 싶다!\n",
    "- numpy() 를 사용해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef8edcd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "print(loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ccd6a5",
   "metadata": {},
   "source": [
    "- requires_grad = True 인 경우, numpy() 모듈 사용 불가능.\n",
    "- requires_grad를 그만두는 torch.no_grad() 나 detach() 중 하나를 사용해야 한다.\n",
    "- 제일 간단한건  `torch.item()` 모듈 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9257a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303557.16782532405\n"
     ]
    }
   ],
   "source": [
    "print(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3461bc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303557.16782532405\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e78d3beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303557.16782532405"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001c0d9",
   "metadata": {},
   "source": [
    "#### w, b 값 재할당 (assign)\n",
    "- assign 대신에 data에 접근해서 값을 수정 \n",
    "- tensor.data = 다른데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a389ba87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - loss : 303557.16782532405\n",
      "1 - loss : 2295406.8070335095\n",
      "2 - loss : 539650.3465805259\n",
      "3 - loss : 2031799.0480018184\n",
      "4 - loss : 825037.614489293\n",
      "5 - loss : 1729338.5293880112\n",
      "6 - loss : 1142547.9143064006\n",
      "7 - loss : 1404591.5434815108\n",
      "8 - loss : 1471707.1735128702\n",
      "9 - loss : 1075681.4629805093\n",
      "10 - loss : 1793099.0528657676\n",
      "11 - loss : 763285.6240031882\n",
      "12 - loss : 2089380.7184846008\n",
      "13 - loss : 487146.5766556782\n",
      "14 - loss : 2343144.693960432\n",
      "15 - loss : 262557.92599439813\n",
      "16 - loss : 2537698.0823960975\n",
      "17 - loss : 101438.17198060958\n",
      "18 - loss : 2661185.157926967\n",
      "19 - loss : 14211.317602769817\n",
      "20 - loss : 2707921.9994103303\n",
      "21 - loss : 7525.19904706802\n",
      "22 - loss : 2675675.686541709\n",
      "23 - loss : 81069.21104010857\n",
      "24 - loss : 2564791.788789602\n",
      "25 - loss : 228917.96366084932\n",
      "26 - loss : 2381109.006384795\n",
      "27 - loss : 442832.0813088454\n",
      "28 - loss : 2136975.3824908794\n",
      "29 - loss : 711779.7100858286\n",
      "30 - loss : 1847747.9191313416\n",
      "31 - loss : 1019522.0964519086\n",
      "32 - loss : 1529184.607560186\n",
      "33 - loss : 1346168.0387774226\n",
      "34 - loss : 1199074.203147269\n",
      "35 - loss : 1672491.2246415007\n",
      "36 - loss : 878193.7348580217\n",
      "37 - loss : 1980857.4521934395\n",
      "38 - loss : 586760.4419117824\n",
      "39 - loss : 2253103.2316752393\n",
      "40 - loss : 340942.1919204403\n",
      "41 - loss : 2471436.9936994677\n",
      "42 - loss : 153939.3660913281\n",
      "43 - loss : 2622636.5970486845\n",
      "44 - loss : 37744.14203665467\n",
      "45 - loss : 2699388.7917338344\n",
      "46 - loss : 700.6071661734998\n",
      "47 - loss : 2697645.6960056038\n",
      "48 - loss : 44259.484071357176\n",
      "49 - loss : 2615933.4034146224\n",
      "50 - loss : 164287.37350926167\n",
      "51 - loss : 2458402.4093612423\n",
      "52 - loss : 354232.462375131\n",
      "53 - loss : 2235873.4331351826\n",
      "54 - loss : 604472.6475191788\n",
      "55 - loss : 1962390.7400079775\n",
      "56 - loss : 899837.6242182989\n",
      "57 - loss : 1652757.666228484\n",
      "58 - loss : 1221175.8978038372\n",
      "59 - loss : 1324267.540316903\n",
      "60 - loss : 1549617.4948726841\n",
      "61 - loss : 997630.0904819567\n",
      "62 - loss : 1867402.8740848855\n",
      "63 - loss : 693375.3070986429\n",
      "64 - loss : 2155770.31531264\n",
      "65 - loss : 428399.37843993935\n",
      "66 - loss : 2395969.8673545173\n",
      "67 - loss : 217062.040303142\n",
      "68 - loss : 2573525.59325746\n",
      "69 - loss : 72830.28396245374\n",
      "70 - loss : 2679572.8876371398\n",
      "71 - loss : 5686.848737095787\n",
      "72 - loss : 2708289.5012052273\n",
      "73 - loss : 18827.05411647827\n",
      "74 - loss : 2656383.4786033714\n",
      "75 - loss : 109933.85145690065\n",
      "76 - loss : 2526277.5870585977\n",
      "77 - loss : 274205.3312619888\n",
      "78 - loss : 2327186.400731247\n",
      "79 - loss : 503526.48430983315\n",
      "80 - loss : 2071729.76720702\n",
      "81 - loss : 783923.95603752\n",
      "82 - loss : 1773619.5136256986\n",
      "83 - loss : 1097137.7092846208\n",
      "84 - loss : 1449498.155367332\n",
      "85 - loss : 1424818.3510045172\n",
      "86 - loss : 1119842.3288666762\n",
      "87 - loss : 1749248.2727586718\n",
      "88 - loss : 805329.8997408964\n",
      "89 - loss : 2051229.898796678\n",
      "90 - loss : 523432.5917650062\n",
      "91 - loss : 2311202.8868211783\n",
      "92 - loss : 289536.6525800037\n",
      "93 - loss : 2513560.516863801\n",
      "94 - loss : 118477.40390027143\n",
      "95 - loss : 2647977.3279681383\n",
      "96 - loss : 21803.516128583087\n",
      "97 - loss : 2706911.1989534223\n",
      "98 - loss : 4421.881076922464\n",
      "99 - loss : 2685267.8999605663\n"
     ]
    }
   ],
   "source": [
    "lr = 0.000003\n",
    "\n",
    "for epoch in range(100):\n",
    "    z = x.mm(w) + b\n",
    "    loss = torch.mean((y - z)**2)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    w.data -= w.grad * lr  # 재할당\n",
    "    b.data -= b.grad * lr\n",
    "    \n",
    "    print(\"{} - loss : {}\".format(epoch, loss.item()))\n",
    "    \n",
    "    w.grad.zero_    # 미분값이 누적되는것을 막기 위함\n",
    "    b.grad.zero_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d3249",
   "metadata": {},
   "source": [
    "#### bakward() 를 안쓰는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9262dd6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - loss : 65839.53900859304\n",
      "1 - loss : 50755.865676375266\n",
      "2 - loss : 39133.245019078175\n",
      "3 - loss : 30177.237486059206\n",
      "4 - loss : 23275.82654223587\n",
      "5 - loss : 17957.511625689407\n",
      "6 - loss : 13859.032929556117\n",
      "7 - loss : 10700.513011777482\n",
      "8 - loss : 8266.309934114777\n",
      "9 - loss : 6390.268957784455\n",
      "10 - loss : 4944.361827146665\n",
      "11 - loss : 3829.9351653279423\n",
      "12 - loss : 2970.968497807291\n",
      "13 - loss : 2308.8802319965343\n",
      "14 - loss : 1798.526033436633\n",
      "15 - loss : 1405.1157487306527\n",
      "16 - loss : 1101.8379473885739\n",
      "17 - loss : 868.0296118091891\n",
      "18 - loss : 687.7658239530382\n",
      "19 - loss : 548.7730402891115\n",
      "20 - loss : 441.59168530294596\n",
      "21 - loss : 358.93084650911146\n",
      "22 - loss : 295.1709895477551\n",
      "23 - loss : 245.98073073900997\n",
      "24 - loss : 208.02149961341522\n",
      "25 - loss : 178.719929262709\n",
      "26 - loss : 156.09243899195513\n",
      "27 - loss : 138.6100383461557\n",
      "28 - loss : 125.09412801961099\n",
      "29 - loss : 114.63618929069429\n",
      "30 - loss : 106.53588416288858\n",
      "31 - loss : 100.25334480456043\n",
      "32 - loss : 95.37239903657719\n",
      "33 - loss : 91.57222467296033\n",
      "34 - loss : 88.60550044186934\n",
      "35 - loss : 86.28156426929111\n",
      "36 - loss : 84.45343114998171\n",
      "37 - loss : 83.00778597021326\n",
      "38 - loss : 81.8572694461774\n",
      "39 - loss : 80.9345316406312\n",
      "40 - loss : 80.18764798332825\n",
      "41 - loss : 79.57658556614636\n",
      "42 - loss : 79.0704790446396\n",
      "43 - loss : 78.64553063444201\n",
      "44 - loss : 78.28339120465124\n",
      "45 - loss : 77.9699122395943\n",
      "46 - loss : 77.69418369912349\n",
      "47 - loss : 77.44779227747571\n",
      "48 - loss : 77.2242495686822\n",
      "49 - loss : 77.01855121524609\n",
      "50 - loss : 76.82683703456617\n",
      "51 - loss : 76.64612899193398\n",
      "52 - loss : 76.47412918816237\n",
      "53 - loss : 76.30906411496947\n",
      "54 - loss : 76.14956458037149\n",
      "55 - loss : 75.99457313399138\n",
      "56 - loss : 75.84327269367738\n",
      "57 - loss : 75.6950315175816\n",
      "58 - loss : 75.54936077809388\n",
      "59 - loss : 75.40588185149\n",
      "60 - loss : 75.26430109819715\n",
      "61 - loss : 75.12439041820804\n",
      "62 - loss : 74.98597225906921\n",
      "63 - loss : 74.84890805677327\n",
      "64 - loss : 74.71308932340952\n",
      "65 - loss : 74.57843077546865\n",
      "66 - loss : 74.44486503550173\n",
      "67 - loss : 74.31233854684861\n",
      "68 - loss : 74.18080842365814\n",
      "69 - loss : 74.05024002203287\n",
      "70 - loss : 73.92060506717507\n",
      "71 - loss : 73.79188020922203\n",
      "72 - loss : 73.66404590961233\n",
      "73 - loss : 73.5370855823015\n",
      "74 - loss : 73.41098493147489\n",
      "75 - loss : 73.28573144076753\n",
      "76 - loss : 73.16131397930205\n",
      "77 - loss : 73.03772249779873\n",
      "78 - loss : 72.91494779413617\n",
      "79 - loss : 72.79298133246186\n",
      "80 - loss : 72.67181510359374\n",
      "81 - loss : 72.55144151725956\n",
      "82 - loss : 72.4318533188866\n",
      "83 - loss : 72.3130435253212\n",
      "84 - loss : 72.19500537514598\n",
      "85 - loss : 72.0777322902533\n",
      "86 - loss : 71.96121784609942\n",
      "87 - loss : 71.84545574865274\n",
      "88 - loss : 71.73043981650486\n",
      "89 - loss : 71.6161639669636\n",
      "90 - loss : 71.50262220521715\n",
      "91 - loss : 71.38980861586799\n",
      "92 - loss : 71.27771735629418\n",
      "93 - loss : 71.16634265142176\n",
      "94 - loss : 71.0556787895853\n",
      "95 - loss : 70.94572011922938\n",
      "96 - loss : 70.8364610462589\n",
      "97 - loss : 70.72789603189104\n",
      "98 - loss : 70.62001959089488\n",
      "99 - loss : 70.51282629013106\n"
     ]
    }
   ],
   "source": [
    "lr = 0.000003\n",
    "\n",
    "for epoch in range(100):\n",
    "    z = x.mm(w) + b\n",
    "    loss = torch.mean((y - z)**2)\n",
    "    \n",
    "    grads = torch.autograd.grad(loss, [w, b])\n",
    "    \n",
    "    w.data -= grads[0] * lr  # 재할당\n",
    "    b.data -= grads[1] * lr\n",
    "    \n",
    "    print(\"{} - loss : {}\".format(epoch, loss.item()))\n",
    "    \n",
    "    w.grad.zero_    # 미분값이 누적되는것을 막기 위함\n",
    "    b.grad.zero_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e3a88",
   "metadata": {},
   "source": [
    "#### optimizer 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5edc2269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 - loss : 70.40631074732381\r",
      "  1 - loss : 1914728.0594768883\r",
      "  2 - loss : 1476436.2486680443\r",
      "  3 - loss : 1138475.2137664608\r",
      "  4 - loss : 877877.9058279223\r",
      "  5 - loss : 676934.740827916\r",
      "  6 - loss : 521990.0950604188\r",
      "  7 - loss : 402514.2990988359\r",
      "  8 - loss : 310388.0649185431\r",
      "  9 - loss : 239350.71732501432\r",
      " 10 - loss : 184574.73264847437\r",
      " 11 - loss : 142337.67125451902\r",
      " 12 - loss : 109769.20910575116\r",
      " 13 - loss : 84656.07718617044\r",
      " 14 - loss : 65291.65289750432\r",
      " 15 - loss : 50359.98199034524\r",
      " 16 - loss : 38846.3494185291\r",
      " 17 - loss : 29968.321712677207\r",
      " 18 - loss : 23122.57466859949\r",
      " 19 - loss : 17843.8928814854\r",
      " 20 - loss : 13773.554828389493\r",
      " 21 - loss : 10634.955022336699\r",
      " 22 - loss : 8214.806576398532\r",
      " 23 - loss : 6348.646748038507\r",
      " 24 - loss : 4909.66045468763\r",
      " 25 - loss : 3800.062233165679\r",
      " 26 - loss : 2944.450981454378\r",
      " 27 - loss : 2284.6858869969374\r",
      " 28 - loss : 1775.9353219507527\r",
      " 29 - loss : 1383.630197874493\r",
      " 30 - loss : 1081.1147372388891\r",
      " 31 - loss : 847.8350139273091\r",
      " 32 - loss : 667.942160378299\r",
      " 33 - loss : 529.2153187642717\r",
      " 34 - loss : 422.2311426283969\r",
      " 35 - loss : 339.7234103692771\r",
      " 36 - loss : 276.08923149836977\r",
      " 37 - loss : 227.0082886793782\r",
      " 38 - loss : 189.14924019149325\r",
      " 39 - loss : 159.94333066481843\r",
      " 40 - loss : 137.409825243394\r",
      " 41 - loss : 120.02140412211745\r",
      " 42 - loss : 106.60037001088472\r",
      " 43 - loss : 96.23861504849508\r",
      " 44 - loss : 88.23590832127343\r",
      " 45 - loss : 82.05231016343676\r",
      " 46 - loss : 77.27147943658089\r",
      " 47 - loss : 73.57238024490046\r",
      " 48 - loss : 70.70746534705728\r",
      " 49 - loss : 68.48585366541653\r",
      " 50 - loss : 66.76035867947661\r",
      " 51 - loss : 65.41748618655697\r",
      " 52 - loss : 64.36972170339641\r",
      " 53 - loss : 63.54958338042856\r",
      " 54 - loss : 62.90503628020343\r",
      " 55 - loss : 62.39595638622044\r",
      " 56 - loss : 61.99140404540884\r",
      " 57 - loss : 61.66752155450909\r",
      " 58 - loss : 61.40591201581196\r",
      " 59 - loss : 61.1923892935299\r",
      " 60 - loss : 61.01601412111054\r",
      " 61 - loss : 60.86835085587626\r",
      " 62 - loss : 60.74289437199162\r",
      " 63 - loss : 60.634628144917414\r",
      " 64 - loss : 60.53968349594152\r",
      " 65 - loss : 60.455076839951005\r",
      " 66 - loss : 60.37850708050611\r",
      " 67 - loss : 60.30819938373015\r",
      " 68 - loss : 60.24278471431154\r",
      " 69 - loss : 60.181206947214\r",
      " 70 - loss : 60.12265124266331\r",
      " 71 - loss : 60.066488816975095\r",
      " 72 - loss : 60.01223435600702\r",
      " 73 - loss : 59.959513177177165\r",
      " 74 - loss : 59.90803590847784\r",
      " 75 - loss : 59.85757896374927\r",
      " 76 - loss : 59.80796948737493\r",
      " 77 - loss : 59.75907374529218\r",
      " 78 - loss : 59.71078817341209\r",
      " 79 - loss : 59.6630324751347\r",
      " 80 - loss : 59.61574429889539\r",
      " 81 - loss : 59.56887513405386\r",
      " 82 - loss : 59.52238714623167\r",
      " 83 - loss : 59.47625073704686\r",
      " 84 - loss : 59.430442662422614\r",
      " 85 - loss : 59.38494458160507\r",
      " 86 - loss : 59.339741938296285\r",
      " 87 - loss : 59.29482309787684\r",
      " 88 - loss : 59.25017868209661\r",
      " 89 - loss : 59.20580105603072\r",
      " 90 - loss : 59.161683932445804\r",
      " 91 - loss : 59.117822066699986\r",
      " 92 - loss : 59.0742110214527\r",
      " 93 - loss : 59.030846985204086\r",
      " 94 - loss : 58.987726632342316\r",
      " 95 - loss : 58.94484701519712\r",
      " 96 - loss : 58.90220548077342\r",
      " 97 - loss : 58.859799606515566\r",
      " 98 - loss : 58.81762715074631\r",
      " 99 - loss : 58.77568601442151\r"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.SGD([w, b], lr = lr) # 미분값을 추적한 파라미터를 인수로 입력\n",
    "\n",
    "for epoch in range(100):    \n",
    "    z = (x.matmul(w) + b)\n",
    "    loss = torch.mean((z - y)**2)  \n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()  # opt 에서 정한 lr 대로 미분값 할당을 작동하라는 의미. 호출만 하면 된다.\n",
    "    print(\"{:3} - loss : {}\".format(epoch, loss.item()), end=\"\\r\")\n",
    "    \n",
    "    opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "303f2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측한 집값 : 24.823975602955297 실제 집값 : 18.2\n"
     ]
    }
   ],
   "source": [
    "y_pred = x.matmul(w) + b\n",
    "print(\"예측한 집값 :\", y_pred[19].item(), \"실제 집값 :\", boston.target[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b512f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
